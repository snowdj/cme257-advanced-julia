{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 7 - Performance\n",
    "\n",
    "Today we'll talk a bit about performance, in particular how to write Julia code with performance in mind, and how to measure performance.  You can find a lot of useful material in Julia's documentation: [FAQ](http://docs.julialang.org/en/release-0.4/manual/faq/), [performance tips](http://docs.julialang.org/en/release-0.4/manual/performance-tips/), [profiling](http://docs.julialang.org/en/release-0.4/manual/profile/).\n",
    "\n",
    "We won't talk too much about performance relative to other languages (see Julia's figure [here](http://julialang.org/benchmarks/), and look around the internet for criticisms), and mostly concern ourselves with writing fast code within Julia.  However, many of these topics apply directly or indirectly to many languages used for scientific computing, so keep an eye out for questions or connections to languages you are familiar with.\n",
    "\n",
    "A few things that we'll pay attention to today are speed and memory allocation.  A few general heuristics are:\n",
    "* Preallocation is good (don't grow arrays dynamically if avoidable)\n",
    "* Type annotations are good (tell the compiler which types you want to instantiate)\n",
    "* Avoid changing the type of variables\n",
    "* Write multiple function methods instead of multiple code paths in a function\n",
    "* Use for-loops over vectorized notation (see [Victor's notebook](../../packages/devectorize.ipynb))\n",
    "\n",
    "It is worth mentioning that if you don't want to worry about this sort of thing, that's OK.  One of the nice things about Julia is that you can use it at a high level without getting bogged down in this sort of analysis.  However, if you use certain functions a lot, or plan on having others use your functions a lot, or want your simulations to finish faster, it may be worth taking a second pass at your code to find optimizations.  Also, if you practice a bit, you will also be able to write code faster the first time around.\n",
    "\n",
    "\n",
    "## Example: The Inner Product \n",
    "\n",
    "You've probably already used the `@time` macro, which is a very easy way to get an idea of speed and memory allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function bad_innerprod(x, y)\n",
    "    @assert length(x) == length(y)\n",
    "    ans = 0\n",
    "    for i = 1:length(x)\n",
    "        ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = randn(n)\n",
    "y = randn(n)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time bad_innerprod(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `@time` macro will tell you both time information and memory allocation information.  The function above is allocating several kilobytes of memory just to do some simple artihmetic.  This may not seem like a big deal, but imagine if this function was a very small component of a much larger program and was called thousands of times.  Here's a better example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function better_innerprod0{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    ans = zero(T)\n",
    "    for i = 1:length(x)\n",
    "        ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time better_innerprod0(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially just by providing type information, we were able to keep the compiler from allocating unneccssary amounts of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function better_innerprod1{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    ans = zero(T)\n",
    "    for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time better_innerprod1(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `@inbounds` macro is saying that the program doesn't need to check that we may try to access a memory location that isn't part of the array.  The complier may be able to infer this in this particular example, but if you have more complicated loops, the macro may give you a noticeable speedup.\n",
    "\n",
    "Now, we separate the inner loop from bounds checking.  If you have more complex logic, breaking your functions into smaller components can speed up evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function fast_innerprod{T}(x::Array{T}, y::Array{T})\n",
    "    ans::T = 0\n",
    "    for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    ans\n",
    "end\n",
    "\n",
    "function better_innerprod2{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    fast_innerprod(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time fast_innerprod(x, y)\n",
    "@time better_innerprod2(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@simd` macro can be used in loops that can be vectorized.  This means no `break`s or `continue`s, and that the loop should not depend on previous loop evaluations.  See more [here](https://en.wikipedia.org/wiki/SIMD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function fast_innerprod2{T}(x::Array{T}, y::Array{T})\n",
    "    ans::T = 0\n",
    "    @simd for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    ans\n",
    "end\n",
    "\n",
    "function better_innerprod3{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    fast_innerprod2(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time fast_innerprod2(x, y)\n",
    "@time better_innerprod3(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the equivalent of the `-ffast-math` compiler optimization flag with `@fastmath`.  Note that this may change the accuracy of your results, or give you an answer that is entirely wrong if you aren't careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function fast_innerprod3{T}(x::Array{T}, y::Array{T})\n",
    "    ans::T = 0\n",
    "    @fastmath @simd for i = 1:length(x)\n",
    "        @inbounds ans += x[i] * y[i]\n",
    "    end\n",
    "    ans\n",
    "end\n",
    "\n",
    "function better_innerprod4{T}(x::Array{T}, y::Array{T})\n",
    "    @assert length(x) == length(y)\n",
    "    fast_innerprod3(x, y)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time fast_innerprod3(x, y)\n",
    "@time better_innerprod4(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you would compute an inner product with Julia's built in dot: (note that this is calling BLAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time x' * y \n",
    "@time dot(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the efficiency of each implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function gflop_innerprod( n, reps )\n",
    "    x = randn(n)\n",
    "    y = randn(n)\n",
    "    s = zero(Float64)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=bad_innerprod(x,y)\n",
    "    end\n",
    "    println(\"GFlop (bad_innerprod)      = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod0(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod0)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod1(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod1)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod2(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod2)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod3(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod3)  = \",2.0*n*reps/time*1E-9)\n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=better_innerprod4(x,y)\n",
    "    end\n",
    "    println(\"GFlop (better_innerprod4)  = \",2.0*n*reps/time*1E-9)  \n",
    "    time = @elapsed for j in 1:reps\n",
    "        s+=dot(x,y)\n",
    "    end\n",
    "    println(\"GFlop (dot)                = \",2.0*n*reps/time*1E-9)\n",
    "        time = @elapsed for j in 1:reps\n",
    "        s+=x' * y\n",
    "    end\n",
    "    println(\"GFlop (array inner prod)   = \",2.0*n*reps/time*1E-9)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gflop_innerprod(2000, 1000)\n",
    "println(\"\")\n",
    "gflop_innerprod(2000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "* Switch the $+$ and $*$ operations in the definition of inner product, and write a function that will compute this modified function for you.  Make one version that is relatively inneficient and one version that is as fast as you can make it.\n",
    "* (if you have time) If you make the binary operations in the defintion of matrix-vector multiplication parameters of your function can you still get reasonable performance?\n",
    "\n",
    "## Profiling Code\n",
    "\n",
    "Julia has a built-in profiler which will allow you to see where your functions are spending most of their time.  If you have a script or function that is taking a long time to complete, this can help you identify where you should focus your optimization efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function test_fn()\n",
    "    A = randn(1000, 1000)\n",
    "    b  = randn(1000, 1)\n",
    "    c = A * b\n",
    "    maximum(c)\n",
    "end\n",
    "\n",
    "test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@profile test_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Profile.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@profile` macro will run the function several times, randomly interrupting the call and looking at the stack.  The first number in each line is the number of times the function was found on the call stack.  The rest of the line gives you information on the function and where to find it in the code base.  The output is indented based on where in the stack the function was found.\n",
    "\n",
    "If you want to increase the number of samples, you can put your function in a for-loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@profile for i = 1:100 test_fn() end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Profile.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to go beyond the built in profiler, there's a pacakge that will graphically interperet the results called [ProfileView](https://github.com/timholy/ProfileView.jl).  You can [track memory allocation](http://docs.julialang.org/en/release-0.4/manual/profile/#man-track-allocation) for each line of code by starting up Julia with `--track-allocation=<setting>`.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "* Use Julia's profiler on the package of your choice.  What's taking the most time?  If you want a starting point, try PyCall.\n",
    "* (if you have time) Use the profiler on your Julia set package.  Where would you focus your efforts if you wanted increased speed?\n",
    "\n",
    "## More speed\n",
    "\n",
    "### [Type Definitions](http://docs.julialang.org/en/release-0.4/manual/performance-tips/#type-declarations)\n",
    "\n",
    "When you declare types, you should (whenever possible) make fields a concrete type, not even a specific abstract type.  If you want to allow for multiple types in the field, parameterize your type.  If there is any ambiguity in what the actual instantiated type will be, the compiler will not be able to allocate space correctly, and will generally miss out on optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type AmbiguousType\n",
    "    x\n",
    "end\n",
    "\n",
    "type StillAmbiguousType\n",
    "    x::Real\n",
    "end\n",
    "\n",
    "type NonAmbiguousType\n",
    "    x::Float64 \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "for T in (AmbiguousType, StillAmbiguousType, NonAmbiguousType)\n",
    "    @time a = Array(T, n)\n",
    "    t1 = @elapsed for i=1:n\n",
    "        a[i] = T(randn())\n",
    "    end\n",
    "    println(\"$t1 seconds to fill array\")\n",
    "    s = T(0)\n",
    "    t2 = @elapsed for i=1:n\n",
    "        s.x += (a[i]).x\n",
    "    end\n",
    "    println(\"$t2 seconds to sum array\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use arrays, you should pre-allocate if possible.  Specific type information is also valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 10000\n",
    "@time a1 = Real[] # Abstract type\n",
    "@time a2 = Float64[] # specific type\n",
    "@time a3 = Array(Real, n) # pre-allocated abstract type\n",
    "@time a4 = Array(Float64, n) # pre-allocated specific type\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = @elapsed for i=1:n\n",
    "    push!(a1,randn())\n",
    "end\n",
    "println(\"$t1 seconds to fill a1\")\n",
    "t2 = @elapsed for i=1:n\n",
    "    push!(a2,randn())\n",
    "end\n",
    "println(\"$t2 seconds to fill a2\")\n",
    "t3 = @elapsed for i=1:n\n",
    "    @inbounds a3[i] = randn()\n",
    "end\n",
    "println(\"$t3 seconds to fill a3\")\n",
    "t4 = @elapsed for i=1:n\n",
    "    @inbounds a4[i] = randn()\n",
    "end\n",
    "println(\"$t4 seconds to fill a4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Subnormal Numbers](http://docs.julialang.org/en/release-0.4/manual/performance-tips/#treat-subnormal-numbers-as-zeros)\n",
    "\n",
    "You can treat sub-normal numbers as zero.  If a number is less than what can be represented using floating point, your computer may still represent it, and incur performance penalites (although this is required for IEEE standards, so be careful).  See [Denormal Numbers](https://en.wikipedia.org/wiki/Denormal_number) on Wikipedia for more info. The following example is from [Julia's documentation](http://docs.julialang.org/en/release-0.4/manual/performance-tips/#treat-subnormal-numbers-as-zeros), and models the heat equation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function timestep{T}( b::Vector{T}, a::Vector{T}, Δt::T )\n",
    "    @assert length(a)==length(b)\n",
    "    n = length(b)\n",
    "    b[1] = 1                            # Boundary condition\n",
    "    for i=2:n-1\n",
    "        b[i] = a[i] + (a[i-1] - T(2)*a[i] + a[i+1]) * Δt\n",
    "    end\n",
    "    b[n] = 0                            # Boundary condition\n",
    "end\n",
    "\n",
    "function heatflow{T}( a::Vector{T}, nstep::Integer )\n",
    "    b = similar(a)\n",
    "    for t=1:div(nstep,2)                # Assume nstep is even\n",
    "        timestep(b,a,T(0.1))\n",
    "        timestep(a,b,T(0.1))\n",
    "    end\n",
    "end\n",
    "\n",
    "heatflow(zeros(Float32,10),2)           # Force compilation\n",
    "for trial=1:6\n",
    "    a = zeros(Float32,1000)\n",
    "    set_zero_subnormals(iseven(trial))  # Odd trials use strict IEEE arithmetic\n",
    "    @time heatflow(a,1000)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Access arrays in column-major order](http://docs.julialang.org/en/release-0.4/manual/performance-tips/#access-arrays-in-memory-order-along-columns)\n",
    "If you need to loop over an array, keep in mind that it is stored in column-major format, so looping over indices in reverse order will allow you to use blocks of memory more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function sum_array1{T}(A::Array{T,3})\n",
    "    s::T = 0\n",
    "    @simd for k=1:size(A,3)\n",
    "        @simd for j=1:size(A,2)\n",
    "            @simd for i=1:size(A,1)\n",
    "                @inbounds s += A[i,j,k]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "function sum_array2{T}(A::Array{T,3})\n",
    "    s::T = 0\n",
    "    @simd for i=1:size(A,1)\n",
    "        @simd for j=1:size(A,2)\n",
    "            @simd for k=1:size(A,3)\n",
    "                @inbounds s += A[i,j,k]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 300\n",
    "A = rand(Int64,n,n,n)\n",
    "@time sum_array1(A)\n",
    "@time sum_array2(A)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = [1 2; 3 4]\n",
    "@show A\n",
    "@show A[:]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Minor Tweaks](http://docs.julialang.org/en/release-0.4/manual/performance-tips/#tweaks)\n",
    "\n",
    "Julia's performance documentation suggests the following optimizations for making very fast inner loops:\n",
    "\n",
    "* Avoid unnecessary arrays. For example, instead of sum([x,y,z]) use x+y+z.\n",
    "* Use `abs2(z)` instead of `abs(z)^2` for complex z. In general, try to rewrite code to use `abs2()` instead of `abs()` for complex arguments. (This would be useful for writing fast Julia Set codes!)\n",
    "* Use `div(x,y)` for truncating division of integers instead of `trunc(x/y)`, `fld(x,y)` instead of `floor(x/y)`, and `cld(x,y)` instead of `ceil(x/y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "@time z = Array(Complex{Float64}, n)\n",
    "t = @elapsed for i = 1:n\n",
    "    @inbounds z[i] = Complex{Float64}(randn(),randn())\n",
    "end\n",
    "println(\"$t sec to initialize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bnd = 0.5\n",
    "s = zero(Int64)\n",
    "t = @elapsed for i = 1:n\n",
    "    @inbounds s += (abs(z[i]) > bnd ? one(Int64) : zero(Int64))\n",
    "end\n",
    "println(\"$t sec using abs()\")\n",
    "bnd2 = bnd*bnd\n",
    "s = zero(Int64)\n",
    "t = @elapsed for i = 1:n\n",
    "    @inbounds s += (abs2(z[i]) > bnd2 ? one(Int64) : zero(Int64))\n",
    "end\n",
    "println(\"$t sec using abs2()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "* Write a standard matrix-vector multiplication function on two arrays.  Can you get close to Julia's default implementation's performance?\n",
    "* How would you modify your routine to do a matrix transpose-vector multiplication routine? \n",
    "* Why do you think [BLAS's gemv](http://www.netlib.org/lapack/explore-html/dc/da8/dgemv_8f.html) takes the arguments that it does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Performance Analysis\n",
    "\n",
    "* [Lint](https://github.com/tonyhffong/Lint.jl) - analyze code for potential improvements\n",
    "* [TypeCheck](https://github.com/astrieanna/TypeCheck.jl) - static analysis of types and function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
