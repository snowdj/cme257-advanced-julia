{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a brief and by no means complete introduction to the POMDPs.jl package.\n",
    "# MDPs are Markov decision processes, described by states S, actions A, and rewards R.\n",
    "# POMDPs are partially-observable Markov decision processes. In POMDPs, states are uncertain, so we add observations O.\n",
    "# Applications of POMDPs include collision avoidance systems, path planning, and long-term infrastructure maintenance.\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Julia v 0.6.0\n",
    "# Packages used: POMDPs.jl (https://github.com/JuliaPOMDP/POMDPs.jl)\n",
    "# Tutorials are available for MDPs (the grid world example - http://nbviewer.jupyter.org/github/sisl/POMDPs.jl/blob/master/examples/GridWorld.ipynb)\n",
    "# and for POMDPs (the tiger problem - http://nbviewer.jupyter.org/github/sisl/POMDPs.jl/blob/master/examples/Tiger.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mRecompiling stale cache file /Users/gitanjali/.julia/lib/v0.6/SortingAlgorithms.ji for module SortingAlgorithms.\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mRecompiling stale cache file /Users/gitanjali/.julia/lib/v0.6/POMDPs.ji for module POMDPs.\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "using POMDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequal (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this example, we will consider how to set up an MDP to calculate optimal decision-making \n",
    "# for a portfolio of buildings in earthquake country.\n",
    "# This example is adapted from the grid world example linked above.\n",
    "\n",
    "# Initialize an MDP type\n",
    "type PortfolioMDP <: MDP{Int64, Int64} # MDP{StateType, ActionType}\n",
    "end\n",
    "\n",
    "# for simplicity, buildings can be damaged or undamaged\n",
    "\n",
    "struct PortfolioState \n",
    "    b1::Int64 # damage state of Building 1\n",
    "    b2::Int64 # damage state of Building 2\n",
    "    done::Bool # terminal state \n",
    "end\n",
    "\n",
    "# checks if the position of two states are the same\n",
    "sequal(s1::PortfolioState, s2::PortfolioState) = s1.b1 == s2.b1 && s1.b1 == s2.b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PortfolioAct (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining binary states - damaged or not damaged - and setting up an initial state constructor\n",
    "PortfolioState(b1::Int64, b2::Int64) = PortfolioState(b1,b2, false)\n",
    "\n",
    "# Defining actions - we can retrofit a building or repair a building, if it is damaged, or do nothing\n",
    "# a1 - action associated with Building 1; a2 - action associated with Building 2\n",
    "PortfolioAct(a1::Int64, a2::Int64) = PortfolioAct(a1,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Portfolio"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the building MDP type - we will use this as a data container\n",
    "type Portfolio <: MDP{PortfolioState, PortfolioAct} # Note that our MDP is parametarized by the state and the action\n",
    "    size::Int64 # number of buildings in the portfolio\n",
    "    reward_states::Vector{PortfolioState} # the states in which agent recieves reward\n",
    "    reward_values::Vector{Float64} # reward values for those states\n",
    "    tprob::Float64 # probability of transitioning to the desired state\n",
    "    gamma::Float64 # discount factor - this expresses how much we value future reward relative to present rewards (from 0 to 1)\n",
    "end\n",
    "\n",
    "# we use key worded arguments so we can change any of the values we pass in \n",
    "function Portfolio(size = 2,\n",
    "                    reward_states::Vector{PortfolioState}=[PortfolioState(0,0), PortfolioState(0,1), PortfolioState(1,0), PortfolioState(1,1)], # reward states\n",
    "                    reward_values::Vector{Float64}=rv = [10000,4000,6000,-100000], # reward values\n",
    "                    tprob::Float64=0.5, # tprob\n",
    "                    gamma::Float64=0.5) # discount factor\n",
    "    return Portfolio(size, rs, rv, tp, gamma)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{PortfolioState,1}:\n",
       " PortfolioState(0, 0, false)\n",
       " PortfolioState(0, 1, false)\n",
       " PortfolioState(1, 0, false)\n",
       " PortfolioState(1, 1, false)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now create a Portfolio mdp instance like this:\n",
    "mdp = Portfolio()\n",
    "mdp.reward_states # mdp contains all the default values from the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.states(mdp::Portfolio)\n",
    "    s = PortfolioState[] # initialize an array of PortfolioStates\n",
    "    # loop over all our states, remember there are two binary variables:\n",
    "    # done (d)\n",
    "    for d = 0:1, b1 = 0:1, b2 = 0:1\n",
    "        push!(s, PortfolioState(b1,b2,d))\n",
    "    end\n",
    "    return s\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PortfolioState(0, 0, false)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp = Portfolio()\n",
    "state_space = states(mdp);\n",
    "state_space[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.actions(mdp::Portfolio) = [0, 1, 2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.transition(mdp::Portfolio, state::PortfolioState, action::Int64)\n",
    "    a = action\n",
    "    x = state.x\n",
    "    y = state.y\n",
    "    \n",
    "    if state.done\n",
    "        return SparseCat([GridWorldState(b1, b2, true)], [1.0])\n",
    "    elseif state in mdp.reward_states\n",
    "        return SparseCat([GridWorldState(b1, b2, true)], [1.0])\n",
    "    end\n",
    "    \n",
    "    prob = 0.7\n",
    "    \n",
    "    return prob\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.reward(mdp::Portfolio, state::PortfolioState, action::Int64, statep::PortfolioState)\n",
    "    if state.done\n",
    "        return 0.0\n",
    "    end\n",
    "    r = 0.0\n",
    "    n = length(mdp.reward_states)\n",
    "    for i = 1:n\n",
    "        if sequal(state, mdp.reward_states[i])\n",
    "            r += mdp.reward_values[i]\n",
    "        end\n",
    "    end\n",
    "    return r\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.n_states(mdp::Portfolio) = 2*mdp.size\n",
    "POMDPs.n_actions(mdp::Portfolio) = 3\n",
    "POMDPs.discount(mdp::Portfolio) = mdp.gamma;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install support tools we'll use for simulation\n",
    "#POMDPs.add(\"POMDPToolbox\")\n",
    "#Pkg.update(\"POMDPToolbox\")\n",
    "\n",
    "#mdp = Portfolio()\n",
    "#mdp.tprob=1.0\n",
    "#sim(mdp, Portfolio(4,1), max_steps=10) do s\n",
    "#    println(\"state is: $s\")\n",
    "#    a = :right\n",
    "#    println(\"moving $a\")\n",
    "#    return a\n",
    "#end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package already installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mCloning DiscreteValueIteration from https://github.com/JuliaPOMDP/DiscreteValueIteration.jl\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPrecompiling module POMDPToolbox.\n",
      "\u001b[39m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: no method matching ordered_vector(::#PortfolioAct, ::POMDPToolbox.##51#52{Portfolio}, ::Array{Int64,1}, ::Int64, ::String)\u001b[0m\nClosest candidates are:\n  ordered_vector(\u001b[91m::Type\u001b[39m, ::Function, ::Any, ::Any, ::Any) at /Users/gitanjali/.julia/v0.6/POMDPToolbox/src/model/ordered_spaces.jl:31\n  ordered_vector(\u001b[91m::Type\u001b[39m, ::Function, ::Any, ::Any, ::Any, \u001b[91m::Any\u001b[39m) at /Users/gitanjali/.julia/v0.6/POMDPToolbox/src/model/ordered_spaces.jl:31\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching ordered_vector(::#PortfolioAct, ::POMDPToolbox.##51#52{Portfolio}, ::Array{Int64,1}, ::Int64, ::String)\u001b[0m\nClosest candidates are:\n  ordered_vector(\u001b[91m::Type\u001b[39m, ::Function, ::Any, ::Any, ::Any) at /Users/gitanjali/.julia/v0.6/POMDPToolbox/src/model/ordered_spaces.jl:31\n  ordered_vector(\u001b[91m::Type\u001b[39m, ::Function, ::Any, ::Any, ::Any, \u001b[91m::Any\u001b[39m) at /Users/gitanjali/.julia/v0.6/POMDPToolbox/src/model/ordered_spaces.jl:31\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1m(::DiscreteValueIteration.##call#2#3)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,1}, ::Bool, ::Type{T} where T, ::Portfolio\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/gitanjali/.julia/v0.6/DiscreteValueIteration/src/vanilla.jl:32\u001b[22m\u001b[22m",
      " [2] \u001b[1mDiscreteValueIteration.ValueIterationPolicy\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Portfolio\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/gitanjali/.julia/v0.6/DiscreteValueIteration/src/vanilla.jl:23\u001b[22m\u001b[22m",
      " [3] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "# first let's load the value iteration module\n",
    "POMDPs.add(\"DiscreteValueIteration\")\n",
    "using DiscreteValueIteration\n",
    "\n",
    "# initialize the problem\n",
    "mdp = Portfolio()\n",
    "\n",
    "# initialize the solver\n",
    "# max_iterations: maximum number of iterations value iteration runs for (default is 100)\n",
    "# belres: the value of Bellman residual used in the solver (defualt is 1e-3)\n",
    "solver = ValueIterationSolver(max_iterations=100, belres=1e-3)\n",
    "\n",
    "# initialize the policy by passing in your problem\n",
    "policy = ValueIterationPolicy(mdp) \n",
    "\n",
    "# solve for an optimal policy\n",
    "# if verbose=false, the text output will be supressed (false by default)\n",
    "solve(solver, mdp, policy, verbose=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
