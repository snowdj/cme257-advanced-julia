{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow.jl\n",
    "\n",
    "This notebook is an introduction to TensorFlow package by using Julia 0.6.2. Packages we used here includes TensorFlow, MNIST, Distributions.      \n",
    "\n",
    "Created by Lijing Wang, based on the TensorFlow.jl instructions and examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"TensorFlow\")\n",
    "Pkg.add(\"MNIST\")\n",
    "Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST dataset\n",
    "\n",
    "This part is based on the tutorial for Package MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoader"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MNIST\n",
    "\n",
    "#Define a type for loading dataset\n",
    "type DataLoader\n",
    "    cur_id::Int\n",
    "    order::Vector{Int}\n",
    "end\n",
    "\n",
    "DataLoader() = DataLoader(1, shuffle(1:60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "next_batch (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the training dataset by the giving batch_size\n",
    "function next_batch(loader::DataLoader, batch_size)\n",
    "    x = zeros(Float32, batch_size, 784)\n",
    "    y = zeros(Float32, batch_size, 10)\n",
    "    for i in 1:batch_size\n",
    "        x[i, :] = trainfeatures(loader.order[loader.cur_id])\n",
    "        label = trainlabel(loader.order[loader.cur_id])\n",
    "        y[i, Int(label)+1] = 1.0\n",
    "        loader.cur_id += 1\n",
    "        if loader.cur_id > 60000\n",
    "            loader.cur_id = 1\n",
    "        end\n",
    "    end\n",
    "    x, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_test_set (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load test set\n",
    "function load_test_set(N=10000)\n",
    "    x = zeros(Float32, N, 784)\n",
    "    y = zeros(Float32, N, 10)\n",
    "    for i in 1:N\n",
    "        x[i, :] = testfeatures(i)\n",
    "        label = testlabel(i)\n",
    "        y[i, Int(label)+1] = 1.0 \n",
    "        #Julia API assumes 1-based indexing\n",
    "    end\n",
    "    x,y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoader(1, [30662, 41926, 20864, 58356, 1826, 38976, 33608, 47393, 12972, 10720  â€¦  30512, 38238, 42194, 17062, 44210, 58204, 2111, 1841, 33052, 56974])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start TensorFlow session\n",
    "In this session if you have GPU to be used, you can link it to your session. \n",
    "\n",
    "#### Julia Code\n",
    "ENV[\"TF_USE_GPU\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 15:02:18.625400: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-01-31 15:02:18.625433: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-01-31 15:02:18.625440: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-01-31 15:02:18.625446: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Session(Ptr{Void} @0x0000000118be7090)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TensorFlow\n",
    "sess = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a softmax regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor placeholder_2:1 shape=unknown dtype=Float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = placeholder(Float32)\n",
    "y_ = placeholder(Float32) #True Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = Variable(zeros(Float32, 784, 10))\n",
    "b = Variable(zeros(Float32, 10))\n",
    "\n",
    "run(sess, global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Class and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor reduce_2:1 shape=unknown dtype=Float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nn.softmax(x*W + b) #Predict Y\n",
    "\n",
    "#Cross Entropy Loss Function\n",
    "cross_entropy = reduce_mean(-reduce_sum(y_ .* log(y), axis=[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with mini-batch and Gradient Descent Optimizer\n",
    "This step may take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = train.minimize(train.GradientDescentOptimizer(.00001), cross_entropy)\n",
    "for i in 1:1000\n",
    "    batch = next_batch(loader, 100)\n",
    "    run(sess, train_step, Dict(x=>batch[1], y_=>batch[2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9093\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = equal(indmax(y,2), indmax(y_, 2))\n",
    "accuracy=reduce_mean(cast(correct_prediction, Float32))\n",
    "testx, testy = load_test_set()\n",
    "\n",
    "println(run(sess, accuracy, Dict(x=>testx, y_=>testy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have one softmax node so we may not get high accuracy model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a multi-layer convolutional network\n",
    "\n",
    "Here we set up CNN model for MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Session(Ptr{Void} @0x0000000117270150)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "\n",
    "session = Session(Graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate weight_variable W and bias_variable b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias_variable (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function weight_variable(shape)\n",
    "    initial = map(Float32, rand(Normal(0, .001), shape...))\n",
    "    return Variable(initial)\n",
    "end\n",
    "\n",
    "function bias_variable(shape)\n",
    "    initial = fill(Float32(.1), shape...)\n",
    "    return Variable(initial)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build 2D convolutional function and perform maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_pool_2x2 (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function conv2d(x, W)\n",
    "    nn.conv2d(x, W, [1, 1, 1, 1], \"SAME\")\n",
    "end\n",
    "\n",
    "function max_pool_2x2(x)\n",
    "    nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], \"SAME\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor reduce_3:1 shape=unknown dtype=Float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions\n",
    "\n",
    "\n",
    "x = placeholder(Float32)\n",
    "y_ = placeholder(Float32)\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "h_conv1 = nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = nn.relu(h_pool2_flat * W_fc1 + b_fc1)\n",
    "\n",
    "keep_prob = placeholder(Float32)\n",
    "h_fc1_drop = nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = nn.softmax(h_fc1_drop * W_fc2 + b_fc2)\n",
    "\n",
    "cross_entropy = reduce_mean(-reduce_sum(y_.*log(y_conv), axis=[2]))\n",
    "\n",
    "train_step = train.minimize(train.AdamOptimizer(1e-4), cross_entropy)\n",
    "\n",
    "correct_prediction = equal(indmax(y_,2), indmax(y_conv, 2))\n",
    "\n",
    "accuracy = reduce_mean(cast(correct_prediction, Float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(session, global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Minibatch, dropout and AdamOptimizer\n",
    "This step may take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 1, training accuracy 0.12\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 101, training accuracy 0.84\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 201, training accuracy 0.96\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 301, training accuracy 0.96\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 401, training accuracy 0.96\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 501, training accuracy 0.98\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 601, training accuracy 0.96\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 701, training accuracy 0.9\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 801, training accuracy 0.96\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mstep 901, training accuracy 0.96\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "for i in 1:1000\n",
    "    batch = next_batch(loader, 50)\n",
    "    if i%100 == 1\n",
    "        train_accuracy = run(session, accuracy, Dict(x=>batch[1], y_=>batch[2], keep_prob=>1.0))\n",
    "        info(\"step $i, training accuracy $train_accuracy\")\n",
    "    end\n",
    "    run(session, train_step, Dict(x=>batch[1], y_=>batch[2], keep_prob=>.5))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mtest accuracy 0.9784\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "testx, testy = load_test_set()\n",
    "test_accuracy = run(session, accuracy, Dict(x=>testx, y_=>testy, keep_prob=>1.0))\n",
    "info(\"test accuracy $test_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah! Now we get good prediction for MNIST by CNN!\n",
    "\n",
    "Here we do not have overfitting because of the dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
